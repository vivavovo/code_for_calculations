{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the code related to calculations in our work. \n",
    "Since it involves downloading Python packages, you can choose to run it on the Colab platform (https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculations for truthfulness & informativeness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "from scipy.stats import poisson\n",
    "import mpmath as mp\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability and KL divergence calculation model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poisson distribution probability model\n",
    "# Prior probability\n",
    "def calc_prob(lambda_val, k_range):\n",
    "    return [poisson.pmf(k, lambda_val) for k in k_range]\n",
    "\n",
    "# Posterior probability\n",
    "def posterior_a (i,prob,prior_dist,max_k):\n",
    "    posterior = [prior_dist[k] / prob if k == i else 0 for k in range(max_k + 1)]\n",
    "    return posterior\n",
    "\n",
    "def posterior_b (a,b,prob,prior_dist,max_k):\n",
    "    posterior = [prior_dist[k] / prob if a <= k <= b else 0 for k in range(max_k + 1)]\n",
    "    return posterior\n",
    "\n",
    "# KL divergence (relative entropy) D_KL(P || Q).\n",
    "def kl_divergence(p_dist, q_dist):\n",
    "    return sum(p * np.log2(p / q) for p, q in zip(p_dist, q_dist) if p > 0 and q > 0)\n",
    "\n",
    "# Normal distribution probability model\n",
    "def cdf(x,mean,sd):\n",
    "    z = (x - mean) / (sd * mp.sqrt(2))\n",
    "    return 0.5 * (1 + mp.erf(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.891547109557455, 0.6990898947627807)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This example uses the Poisson distribution\n",
    "lambda_reading = 5\n",
    "max_k = 20  # Truncation range\n",
    "\n",
    "# Prior probability\n",
    "prior_dist = calc_prob(lambda_reading, range(max_k + 1))\n",
    "\n",
    "# Posterior probability1\n",
    "posterior_B1 = posterior_a(1, poisson.pmf(1, lambda_reading), prior_dist, max_k)\n",
    "\n",
    "# Posterior probability2: No more than five books (k ∈ [0, 5]).\n",
    "total_prob_B2 = sum(prior_dist[k] for k in range(0, 6))\n",
    "posterior_B2 = posterior_b(0, 5, total_prob_B2, prior_dist, max_k)\n",
    "\n",
    "# KL divergence (relative entropy) \n",
    "kl_B1 = kl_divergence(posterior_B1, prior_dist)\n",
    "kl_B2 = kl_divergence(posterior_B2, prior_dist)\n",
    "\n",
    "kl_B1, kl_B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples (10) - (14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1: prob: 0.049787068367863944, 0.0024787521766663585, KL: 4.328085122666891, 8.656170245333781\n",
      "Line 2: prob: 0.049787068367863944, 0.00012340980408667956, KL: 4.328085122666891, 12.984255368000671\n",
      "Line 3: prob: 0.716933784497241, 0.11445642280019108, KL: 0.33006064857695866, 2.990573791037185\n",
      "Line 4: prob: 0.14936120510359185, 0.0011106882367801166, KL: 2.7431226219457336, 9.814330366558357\n",
      "Line 5: prob: 0.22404180765538775, 0.04461753917999444, KL: 2.158160121224578, 4.486245243891468\n"
     ]
    }
   ],
   "source": [
    "#using the Poisson distribution\n",
    "max_k = 20 \n",
    "lambda_a, lambda_b, lambda_c = 3, 6, 9\n",
    "\n",
    "prob_a1 = poisson.pmf(0, lambda_a)\n",
    "prob_b1 = poisson.pmf(0, lambda_b)\n",
    "prob_a2 = poisson.pmf(0, lambda_a)\n",
    "prob_b2 = poisson.pmf(0, lambda_c)\n",
    "prob_a3 = sum(poisson.pmf(k, lambda_a) for k in range(2, 6))\n",
    "prob_b3 = sum(poisson.pmf(k, lambda_c) for k in range(2, 6))\n",
    "prob_a4 = poisson.pmf(1, lambda_a)\n",
    "prob_b4 = poisson.pmf(1, lambda_c)\n",
    "prob_a5 = poisson.pmf(2, lambda_a)\n",
    "prob_b5 = poisson.pmf(2, lambda_b)\n",
    "\n",
    "# Prior probability\n",
    "prior_dist_a = calc_prob(lambda_a, range(max_k + 1))\n",
    "prior_dist_b = calc_prob(lambda_b, range(max_k + 1))\n",
    "prior_dist_c = calc_prob(lambda_c, range(max_k + 1))\n",
    "\n",
    "# Posterior probability\n",
    "posterior_A1_dist = posterior_a(0, prob_a1, prior_dist_a, max_k)\n",
    "posterior_B1_dist = posterior_a(0, prob_b1, prior_dist_b, max_k)\n",
    "posterior_A2_dist = posterior_a(0, prob_a2, prior_dist_a, max_k)\n",
    "posterior_B2_dist = posterior_a(0, prob_b2, prior_dist_c, max_k)\n",
    "posterior_A3_dist = posterior_b(3, 5, prob_a3, prior_dist_a, max_k)\n",
    "posterior_B3_dist = posterior_b(3, 5, prob_b3, prior_dist_c, max_k)\n",
    "posterior_A4_dist = posterior_a(1, prob_a4, prior_dist_a, max_k)\n",
    "posterior_B4_dist = posterior_a(1, prob_b4, prior_dist_c, max_k)\n",
    "posterior_A5_dist = posterior_a(2, prob_a5, prior_dist_a, max_k)\n",
    "posterior_B5_dist = posterior_a(2, prob_b5, prior_dist_b, max_k)\n",
    "\n",
    "# KL divergence\n",
    "kl_values = [\n",
    "    (kl_divergence(posterior_A1_dist, prior_dist_a), kl_divergence(posterior_B1_dist, prior_dist_b)),\n",
    "    (kl_divergence(posterior_A2_dist, prior_dist_a), kl_divergence(posterior_B2_dist, prior_dist_c)),\n",
    "    (kl_divergence(posterior_A3_dist, prior_dist_a), kl_divergence(posterior_B3_dist, prior_dist_c)),\n",
    "    (kl_divergence(posterior_A4_dist, prior_dist_a), kl_divergence(posterior_B4_dist, prior_dist_c)),\n",
    "    (kl_divergence(posterior_A5_dist, prior_dist_a), kl_divergence(posterior_B5_dist, prior_dist_b))\n",
    "]\n",
    "\n",
    "# result\n",
    "for i, (kl_a, kl_b) in enumerate(kl_values):\n",
    "    prob_a = [prob_a1, prob_a2, prob_a3, prob_a4, prob_a5][i]\n",
    "    prob_b = [prob_b1, prob_b2, prob_b3, prob_b4, prob_b5][i]\n",
    "    print(f'Line {i+1}: prob: {prob_a}, {prob_b}, KL: {kl_a}, {kl_b}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X < 1) = 2.9418e-07, D_KL = 21.6968 bits\n",
      "P(X < 600) = 2.2750e-02, D_KL = 5.4580 bits\n"
     ]
    }
   ],
   "source": [
    "# using Normal distribution probability model\n",
    "# Increase calculation accuracy\n",
    "mp.mp.dps = 50\n",
    "\n",
    "mean = 1000\n",
    "sd = 200\n",
    "\n",
    "for c in [1, 600]:\n",
    "    p = cdf(c,mean,sd)\n",
    "    dkl_bits = -mp.log(p, 2)  # KL（bits）\n",
    "    print(f\"P(X < {c}) = {float(p):.4e}, D_KL = {float(dkl_bits):.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples (18) and (19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: Only 1 book (k = 1)\n",
      "Probability A2 (only 1): 0.14936120510359185, Probability B2 (only 1): 0.049990484422090385\n",
      "KL divergence A2: 2.7431226219457336, KL divergence B2: 4.322202682558022\n",
      "\n",
      "Scenario: Exactly 2 books (k = 2)\n",
      "Probability A3 (exactly 2): 0.010734804092880374, Probability B3 (exactly 2): 0.0004423832894396311\n",
      "KL divergence A3: 6.541560327111708, KL divergence B3: 11.142415489225248\n"
     ]
    }
   ],
   "source": [
    "#using the Poisson distribution model\n",
    "\n",
    "n2, n3 = 15, 40  # Scenario parameters\n",
    "# λ\n",
    "lambda_A2, lambda_B2 = n2 * 0.20, n2 * 0.30\n",
    "lambda_A3, lambda_B3 = n3 * 0.20, n3 * 0.30\n",
    "\n",
    "# prob\n",
    "prob_A2 = poisson.pmf(1, lambda_A2)\n",
    "prob_B2 = poisson.pmf(1, lambda_B2)\n",
    "prob_A3 = poisson.pmf(2, lambda_A3)\n",
    "prob_B3 = poisson.pmf(2, lambda_B3)\n",
    "\n",
    "# Prior probability\n",
    "prior_A2_dist = calc_prob(lambda_A2, range(n2 + 1))\n",
    "prior_B2_dist = calc_prob(lambda_B2, range(n2 + 1))\n",
    "prior_A3_dist = calc_prob(lambda_A3, range(n3 + 1))\n",
    "prior_B3_dist = calc_prob(lambda_B3, range(n3 + 1))\n",
    "\n",
    "# Posterior probability\n",
    "posterior_A2_dist = posterior_a(1, prob_A2, prior_A2_dist, n2)\n",
    "posterior_B2_dist = posterior_a(1, prob_B2, prior_B2_dist, n2)\n",
    "posterior_A3_dist = posterior_a(2, prob_A3, prior_A3_dist, n3)\n",
    "posterior_B3_dist = posterior_a(2, prob_B3, prior_B3_dist, n3)\n",
    "\n",
    "# KL\n",
    "kl_A2 = kl_divergence(posterior_A2_dist, prior_A2_dist)\n",
    "kl_B2 = kl_divergence(posterior_B2_dist, prior_B2_dist)\n",
    "kl_A3 = kl_divergence(posterior_A3_dist, prior_A3_dist)\n",
    "kl_B3 = kl_divergence(posterior_B3_dist, prior_B3_dist)\n",
    "\n",
    "\n",
    "# result\n",
    "print(f\"Scenario: Only 1 book (k = 1)\")\n",
    "print(f\"Probability A2 (only 1): {prob_A2}, Probability B2 (only 1): {prob_B2}\")\n",
    "print(f\"KL divergence A2: {kl_A2}, KL divergence B2: {kl_B2}\")\n",
    "print()\n",
    "print(f\"Scenario: Exactly 2 books (k = 2)\")\n",
    "print(f\"Probability A3 (exactly 2): {prob_A3}, Probability B3 (exactly 2): {prob_B3}\")\n",
    "print(f\"KL divergence A3: {kl_A3}, KL divergence B3: {kl_B3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples (20) and (21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial lambda for event (P(X=0) = 0.5): 0.6931\n",
      "Event probability for narrow domain (P(X=0)): 0.5000\n",
      "Event probability for wide domain (P(X=0)): 0.2500\n",
      "KL divergence (narrow domain): 1.0000 bits\n",
      "KL divergence (wide domain): 2.0000 bits\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find lambda where P(X=0) = 0.5\n",
    "lambda_initial = -np.log(0.5)  # Solve for lambda where P(X=0) = 0.5\n",
    "\n",
    "# Step 2: Calculate event probability for X=0 (P(X=0) for narrow domain)\n",
    "prob_narrow = poisson.pmf(0, lambda_initial)\n",
    "\n",
    "# Step 3: Expand the domain by doubling lambda\n",
    "lambda_expanded = lambda_initial * 2\n",
    "\n",
    "# Step 4: Calculate event probability for X=0 (P(X=0) for wide domain)\n",
    "prob_wide = poisson.pmf(0, lambda_expanded)\n",
    "\n",
    "# Step 5: Calculate KL divergence for the initial domain (narrow domain)\n",
    "prior_dist_narrow = calc_prob(lambda_initial, range(21))  # Using a truncation range of 21\n",
    "posterior_narrow = [prior_dist_narrow[k] / prob_narrow if k == 0 else 0 for k in range(21)]\n",
    "kl_narrow = kl_divergence(posterior_narrow, prior_dist_narrow)\n",
    "\n",
    "# Step 6: Calculate KL divergence for the expanded domain (wide domain)\n",
    "prior_dist_wide = calc_prob(lambda_expanded, range(21))\n",
    "posterior_wide = [prior_dist_wide[k] / prob_wide if k == 0 else 0 for k in range(21)]\n",
    "kl_wide = kl_divergence(posterior_wide, prior_dist_wide)\n",
    "\n",
    "# Output results\n",
    "print(f\"Initial lambda for event (P(X=0) = 0.5): {lambda_initial:.4f}\")\n",
    "print(f\"Event probability for narrow domain (P(X=0)): {prob_narrow:.4f}\")\n",
    "print(f\"Event probability for wide domain (P(X=0)): {prob_wide:.4f}\")\n",
    "print(f\"KL divergence (narrow domain): {kl_narrow:.4f} bits\")\n",
    "print(f\"KL divergence (wide domain): {kl_wide:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6138506250143121,\n",
       " 0.8250199170058297,\n",
       " 0.6021530407882428,\n",
       " 0.26001483156529065)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the Poisson distribution model\n",
    "n1 = 40  # DE-quantifier\n",
    "lambda_A1, lambda_B1 = n1 * 0.5, n1 * 0.6\n",
    "\n",
    "# prob\n",
    "prob_A1_few = sum(poisson.pmf(k, lambda_A1) for k in range(19, 33))\n",
    "prob_B1_few = sum(poisson.pmf(k, lambda_B1) for k in range(19, 33))\n",
    "\n",
    "# Prior probability\n",
    "prior_A1_dist = calc_prob(lambda_A1, range(n1 + 1))\n",
    "prior_B1_dist = calc_prob(lambda_B1, range(n1 + 1))\n",
    "\n",
    "# Posterior probability\n",
    "posterior_A1_dist = posterior_b(20, 32, prob_A1_few, prior_A1_dist, n1)\n",
    "posterior_B1_dist = posterior_b(20, 32, prob_B1_few, prior_B1_dist, n1)\n",
    "\n",
    "# KL\n",
    "kl_A1 = kl_divergence(posterior_A1_dist, prior_A1_dist)\n",
    "kl_B1 = kl_divergence(posterior_B1_dist, prior_B1_dist)\n",
    "\n",
    "# RESULT\n",
    "print(f\"Scenario: A bunch of books (k ∈ [20, 32])\")\n",
    "print(f\"Probability A1 (a bunch of): {prob_A1_few}\")\n",
    "print(f\"Probability B1 (a bunch of): {prob_B1_few}\")\n",
    "print(f\"KL divergence A1: {kl_A1}\")\n",
    "print(f\"KL divergence B1: {kl_B1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability P(X=20): 9.5367431640625e-07\n",
      "KL divergence: 20.0\n"
     ]
    }
   ],
   "source": [
    "#Binomial distribution.\n",
    "\n",
    "# Assume there are 20 text messages starting with \"Hi\", and each has a 50% probability of being deleted.\n",
    "n = 20\n",
    "p = 0.5\n",
    "\n",
    "# Prior probability\n",
    "prior_prob = binom.pmf(20, n, p)  # P(X=20)\n",
    "\n",
    "# After the event occurs, we know that X=20 is certain, and the posterior distribution degenerates into a point distribution: P(X=20) = 1.\n",
    "# Relative entropy D_KL(P || Q) = -log2(prior_prob).\n",
    "KL = -np.log2(prior_prob)\n",
    "\n",
    "# result\n",
    "print(f\"Prior probability P(X=20): {prior_prob}\")\n",
    "print(f\"KL divergence: {KL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculation for Relevance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Loading the model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_sim(sentence1, sentence2):\n",
    "    return util.cos_sim(model.encode(sentence1, convert_to_tensor=True), \n",
    "                        model.encode(sentence2, convert_to_tensor=True)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples in Table 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat is sitting on the mat.\",\n",
    "    \"A cat is resting on a mat.\",\n",
    "    \"The sun is shining brightly in the sky.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")\n",
    "\n",
    "sentences = [\n",
    "    \"I enjoy reading science fiction novels.\",\n",
    "    \"Science fiction books are my favorite.\",\n",
    "    \"She bought a new dress for the party.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")\n",
    "\n",
    "sentences = [\n",
    "    \"The conference will be held in Paris.\",\n",
    "    \"The event is scheduled to take place in Paris.\",\n",
    "    \"He is learning to play the guitar.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")"
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example (9)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Anyone who gives a damn about the environment\",\n",
    "    \"shops at Ikea\",\n",
    "]\n",
    "# result\n",
    "cosine_sim = cosine_sim(sentences[0], sentences[1])\n",
    "\n",
    "print(f\"Cosine similarity (relevance): {cosine_sim:.4f}\")\n",
    "\n",
    "sentences = [\n",
    "    \"This project is quite difficult to handle.\",\n",
    "    \"it’s something that would challenge anyone.\",\n",
    "]\n",
    "# result\n",
    "cosine_sim = cosine_sim(sentences[0], sentences[1])\n",
    "\n",
    "print(f\"Cosine similarity (relevance): {cosine_sim:.4f}\")\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All relevance calculations in the main text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 17\n",
    "sentences = [\n",
    "    \"I lost all my money! \",\n",
    "    \"I don't have any money now.\",\n",
    "    \"I don't have enough money now.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")\n",
    "\n",
    "# result\n",
    "sentences = [\n",
    "    \"Most of what she said was complete nonsense.\",\n",
    "    \" I mean, the only thing she said that made any sense was the name 'Gerard.'\",\n",
    "    \"I mean, the only thing she said that made sense was the name 'Gerard.'\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")\n",
    "\n",
    "#example 19\n",
    "sentences = [\n",
    "    \"did anyone do the assigned reading for today’s seminar?\",\n",
    "    \"Exactly two students did any reading.\",\n",
    "    \"Exactly two students did some reading.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")\n",
    "\n",
    "#example 22\n",
    "sentences = [\n",
    "    \"Do you think it’s because of too much homework?\",\n",
    "    \"Yeah, could be. I heard a bunch of teachers assigned any homework last night.\",\n",
    "    \"Yeah, could be. I heard a bunch of teachers assigned homework last night.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")\n",
    "\n",
    "#example 23\n",
    "sentences = [\n",
    "    \"Didn't you get John's text?\",\n",
    "    \"No. I delete any text that begins 'Hi.'\",\n",
    "    \"No. I delete any texts.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")\n",
    "\n",
    "#example 24\n",
    "sentences = [\n",
    "    \"Oscar had been fishing many times\",\n",
    "    \"he learned anything about fishing techniques.\",\n",
    "    \"Tina introduced anything about astrophysics.\"\n",
    "]\n",
    "# result\n",
    "cosine_sim_1 = cosine_sim(sentences[0], sentences[1])\n",
    "cosine_sim_2 = cosine_sim(sentences[0], sentences[2])\n",
    "\n",
    "print(f\"Cosine similarity (relevance1): {cosine_sim_1:.4f}\")\n",
    "print(f\"Cosine similarity (relevance2): {cosine_sim_2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
